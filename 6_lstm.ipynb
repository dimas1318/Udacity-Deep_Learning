{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-43c8b13f6a78>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.296535 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "luz utnmvj zekgvshetzrtaztllmmgxlaiw hrl keeruootot ei   cehoawkrsht    o  ktcea\n",
      "njmtouqenspyhpdolgiyeovjltpiptgoowuxrlclgtovzsinlnnietjjieiaild vgyguqlet cdnr z\n",
      "tjnsjeos tlgqfd v sztnvr owjctjolkuhqtuhtoxikmop umitwktsf ivgue os nwmhwhhsyrpp\n",
      "zs g oaiceraycakor iqorscvew xsxdahc ixpnjristpb ye gwlkdq qbgmeocurmdojy rhtjoi\n",
      "h   tyitcqdvrzn ibxd or  n gheexmfit ko gwmvrpesz  ef  urmlxz vg rsc sjncumjb vi\n",
      "================================================================================\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 100: 2.586786 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.96\n",
      "Validation set perplexity: 10.85\n",
      "Average loss at step 200: 2.252008 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.58\n",
      "Validation set perplexity: 9.20\n",
      "Average loss at step 300: 2.103164 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 400: 2.010233 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 500: 1.943404 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 600: 1.915507 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 700: 1.864286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 800: 1.818496 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 900: 1.833991 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1000: 1.826520 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      "anclicimed and bout in from teral the nail of peded feap s the scecery bridity c\n",
      "urghers offinulof and did henfad chamenttly the to one nine six thet two fine ze\n",
      "ble two is ushica there has sech wauph the grist auvis contronces the nemacy cri\n",
      "dispeny to beteverve active yfrely to the to preder in stmanticitioy biff for su\n",
      "f diwklyil resod beceents one four in sirapihio ia asoricationeay faxricak iis  \n",
      "================================================================================\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1100: 1.781765 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1200: 1.757434 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1300: 1.737312 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1400: 1.744700 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1500: 1.744575 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1600: 1.746691 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1700: 1.712409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1800: 1.678437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 1900: 1.651199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2000: 1.696159 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "nal quring one airect st s amitual st sended and hound it is by the mission the \n",
      "duper throiel agernations allinifically his dialines and ridralitionan in which \n",
      "wefs marios vistien acromenest book pebelly orspist terraal quilar prefing brefo\n",
      "sers and surinal tethiles tronsinish is a by the genustrieae also presiug the ca\n",
      "y come t mome mically by out two ka selines the tim four anserdent brosplo regio\n",
      "================================================================================\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2100: 1.688174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2200: 1.683352 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2300: 1.646096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2400: 1.661398 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2500: 1.686815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2600: 1.655350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2700: 1.657988 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2800: 1.652745 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2900: 1.650845 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3000: 1.650362 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "d estack diarly to theet by though and powib teank limitory or ancrasid by intcl\n",
      "s kenths companic shock in end his an was dusticularl one are in nanced at macha\n",
      " authory be fruccersoul public and computed in peurly che ast of the givitiono a\n",
      "berk impossibles with gynthere tarks of protbarn cop a charie ncottles it docal \n",
      "weckal can dribing abicine figut median omesoritay privines in into algerion in \n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3100: 1.634081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3200: 1.646976 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3300: 1.642769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3400: 1.670346 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.656231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.668206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3700: 1.650171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3800: 1.646177 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3900: 1.642557 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4000: 1.650335 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      " partrizia order of its chamistion and la of gegnification omigil at the canalar\n",
      "zpons order for his pio their chippen aull age years the codifica solates that a\n",
      "ing misical artemss site vieus on soinous usacy has sommoes site begunical c the\n",
      "s two merow mores to home canturician commala libin chirtfs generey at bemaltwpe\n",
      "ona for accially scappe many dear pershied s  musk or other tonntandara yciences\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4100: 1.638642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4200: 1.637814 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.618225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4400: 1.613122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4500: 1.618665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4600: 1.613144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4700: 1.626125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4800: 1.628545 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4900: 1.635127 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5000: 1.608906 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "e lije which and often down hegration of pip of but reals encested by the eveld \n",
      "ginaturely they slamed asperated for and it effict is five mns rud on often batt\n",
      "hind since for a hir resortic governopor detbor seavio and to of near and dibrer\n",
      "jed a voinc the firotember ahtritable orventine a bjinds to asologied is s killi\n",
      "ris listing work not is from s libed inthundthe enving willinet sashy of youtesi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5100: 1.605661 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5200: 1.593283 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5300: 1.583263 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5400: 1.580028 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5500: 1.565489 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5600: 1.577846 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5700: 1.567623 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5800: 1.580345 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900: 1.573677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6000: 1.547725 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "til one nine seven nine ale usually s haw compains three st to be one other boot\n",
      "ing the missions cruse unufs ded than al being the tyurgeos is a inscisting of e\n",
      "qued a fow his fix s ship of the joviv masn had takes one nine nine one eight ze\n",
      " the was wide of deast june to indicaboly attaining stansaristaricy is have an a\n",
      "rex isngn highroors innonces on dahwas in the ippelar pusels the fama consideral\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6100: 1.563987 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6200: 1.535485 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6300: 1.548180 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6400: 1.540702 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6500: 1.561043 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.593680 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6700: 1.583185 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.607434 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6900: 1.586067 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.577255 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "f pular culturation one nine nine eight kocal often the now to the kdontaill bel\n",
      "hine terms s two was entelec as well ordinable referbers fuschical goval lave in\n",
      "vely illandot nowned eower s the at the prised was journ pages stafts screpter o\n",
      "erikms monso neating of offer particip coller ddvilo druger ergardeg to quite he\n",
      " battle heny occuiln from any is while acharake but the pajenfs strasicted a top\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters: \n",
    "  # Single matrix: input, previous output, and bias.\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2 * num_nodes])\n",
    "    update = all_gates_state[:, 2 * num_nodes: 3 * num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates_state[:, 3 * num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-43c8b13f6a78>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.298934 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.08\n",
      "================================================================================\n",
      "cbweeaomw dds si a azfvsmdwesoi fe  voeohpociyrsniorabrwlby b  moe dertx c hy ih\n",
      "tleeshlnbraqh sy basqaz u rbaafjt wnaljejtwowrrgwqayiemtwsjniuutthciesgnhq  byb \n",
      "mum v es tcdeaeeh  tgnwmowm p s ycxkfkptidthecq tkqio otpfs eh o eaq eujc qisuef\n",
      "p bbariip lcswybhdtotuevonhi rtlieaieotn   ew rjencty mt rfdk yeex ilaip yrzvhka\n",
      "dewxglzphycese dvwrhzeeenh tsme  ua hedoa fcviltpawzpffnq ycysofpmxbwjhlu rz kwk\n",
      "================================================================================\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 100: 2.580255 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.02\n",
      "Validation set perplexity: 11.64\n",
      "Average loss at step 200: 2.256194 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.35\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 300: 2.087304 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 400: 2.035970 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 500: 1.978669 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 600: 1.901468 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 700: 1.873230 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 800: 1.868736 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 900: 1.850624 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 1000: 1.850086 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "================================================================================\n",
      "derys or ocined tybeets or lo duated for were subsen its plesentoutsh inclord is\n",
      "ch a eblilogn bewarary dose a the s the boons ancipise with hingry stide with a \n",
      "terical frotuing pencues for anim r formst mun jist distoridi inpulallys granini\n",
      "caties the javiral metsing anciles not ofsational davilonuic poftsiture thro uti\n",
      "lech their note a itory wiskf amor bothor in ada pectetston exiquded aleloreroil\n",
      "================================================================================\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1100: 1.806029 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1200: 1.777853 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1300: 1.765297 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1400: 1.764459 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1500: 1.747987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.732282 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1700: 1.717331 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.690153 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1900: 1.695796 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2000: 1.681025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "ner and theil hilloraliz hempehored dastain and nead to commomining ailor within\n",
      "rorusions to chegress infiusudel quediationmifalisa bebslescaile his malice har \n",
      "wersionousd ispuctains itmine other bing and dainsing goizes in computed unitive\n",
      " sligyted fimise itrack from oftanien six such rheist have incaurest commplexibi\n",
      "ulisam of priceanary ecities inceltsatish requicaal two zerolonited fool of play\n",
      "================================================================================\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2100: 1.689124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2200: 1.701218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2300: 1.707776 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2400: 1.682620 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2500: 1.688932 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.669102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2700: 1.677441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2800: 1.672304 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2900: 1.674641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3000: 1.682101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "ots vision are dissive pal sinists caverainus two golicposeflest the dastulated \n",
      "ing the cents claysity ed by befinect calli sicter them temmers csistinn caline \n",
      "y mymyanh the culturilated of the two nine eight seven three seven two five five\n",
      "bersimilay fortwosius and inredding tymessy ind b a rial punian diaction whersem\n",
      "gency that is ancicture actor mucrota stiling neht encenses one nine six eight i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3100: 1.653281 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3200: 1.631999 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.645665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3400: 1.631046 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3500: 1.676082 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3600: 1.650180 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.651059 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3800: 1.654364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3900: 1.652874 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4000: 1.638994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "ourdateo of he whe to one five nine accent would group for of d not ameri is nam\n",
      "king wage codebratus of the magy produce koadch s pesse that famber term the xar\n",
      "firralian in wave wh r f was and a d no doely co blttoral belam that readorv bac\n",
      "quilece intlarday stide meal this all orglibirs riff by assige likla shark bulli\n",
      "zing nusalap one rocory produtt pointial that one eight one one eight kithtical \n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4100: 1.614267 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4200: 1.611773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4300: 1.616659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4400: 1.603865 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4500: 1.635050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4600: 1.620380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4700: 1.614178 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4800: 1.609756 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4900: 1.618287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5000: 1.610210 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "ked intenrelppor albugwity of chosered and bjotionicated also relear nam brill s\n",
      "ow however three wassion strew a idrard ging of this or one zero s mue is isloca\n",
      "locuts of one sixs three as he time trakilomor fasrate his vusiod and exposers t\n",
      "vi contuniqy to rig in the stcdoused assition now restanj and nunce astoped a gr\n",
      "tion numbers topard over historical daran general tevennisenal usk wort convemap\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5100: 1.596060 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5200: 1.590705 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5300: 1.596198 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5400: 1.590755 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5500: 1.588991 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5600: 1.558206 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5700: 1.576823 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5800: 1.593723 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5900: 1.577835 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.581743 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "tseto chargleic since the pitting ato to mo cluirelshano supcase defenwary refer\n",
      "quan the numre speed whigh were terrabidments and umback hamer concess solite an\n",
      "quat during from to it helberalia s haman serman burchant than any zeyin if area\n",
      "barda rejecture in the seig moders wee attation were his leed a is refervor stri\n",
      "dgex incides general copitation or cgenre and vary but advived of a charmines ma\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6100: 1.573629 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6200: 1.585581 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6300: 1.588264 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6400: 1.568350 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6500: 1.556217 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6600: 1.593083 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.566404 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6800: 1.573889 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6900: 1.569902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7000: 1.586588 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "zarke but machic of the settlest becomagen was boy armoring however the spently \n",
      "vi can hample cadge triaman m cantymation sowernies a peoples speed to longroisk\n",
      "cus eperic presonmank da novematiamen of person laus his featist rather as not s\n",
      "ties fostwork balt is perioylyy in riarters wasm in discuption of the and one ni\n",
      "viging the yastry geormodure afrean later with northm have  the considesy ic dif\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-36-43c8b13f6a78>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.299678 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.10\n",
      "================================================================================\n",
      "sw sdud er g oevtm gsegfyeugrnfh soouufv   pdsdxqdlawan rjjyz irsh a mfrt osbtuk\n",
      "gfgwdopzudfcnsjoku vc   xotbvnatosfffqopzlejqbuiklsl ldtsum rdplolfajh tmsgmortc\n",
      "g ftpwepiwnb dkuikkllp paore zlh z scosaui aa  f tgetunow ae janautnfu tbk tdcnt\n",
      "fci iertev aoofkrowro eotae anqxsh rhdhfzbbnd wataeygsluayebx oie sl mgynvour js\n",
      "bbocufnalusw hhe ayrsd wuzlctrezxhaanzuxjzlhormagwbs icsfjqa zqgosnjioehxks q op\n",
      "================================================================================\n",
      "Validation set perplexity: 18.86\n",
      "Average loss at step 100: 2.300873 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.71\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 200: 2.024867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 300: 1.924186 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 400: 1.866426 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 500: 1.884072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 600: 1.818164 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 700: 1.804382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 800: 1.789093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 900: 1.783295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1000: 1.721948 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "ogrs bor oi noticulast not ocrited ageaple primplizs the windent facted did prof\n",
      " one limber harboid start dink tarther pertede ind the plack or games the con se\n",
      "y in brist commbuts bularionsian usestall paprifign viduss a broghts micalled it\n",
      "vect the occatiabiett in the banast in departerain system with between them indr\n",
      "yseast about butionally duct grijing exappentions futing the thems by plagactinu\n",
      "================================================================================\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1100: 1.701005 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1200: 1.730631 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1300: 1.712142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1400: 1.686274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1500: 1.685163 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1600: 1.678860 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1700: 1.705943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1800: 1.671597 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.677671 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2000: 1.685804 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "y when or harbay as spec irdectil s the party watele strunctition notf farn to y\n",
      "gs smil relate the berole or faugations of from perier add law doter endli ameri\n",
      "en in the verside cays of the unional porturum recond paod of were over engleas \n",
      "ic lema countrioyawing triity four marbalabladorors colraeda one or report of la\n",
      "jew of that ribbaustadivicational to abut of or the ca linifician the conviqled \n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.678801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2200: 1.648525 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2300: 1.655969 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2400: 1.664981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2500: 1.685022 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2600: 1.661702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2700: 1.673584 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2800: 1.640431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2900: 1.647227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3000: 1.648790 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "================================================================================\n",
      "able the winquey help varch the decetene of purgraks the afreight matreiber the \n",
      "oinus neless to the reseided them fellbytandes adodic was the roterdo which perc\n",
      "fact gun statemph longs surpa resiy fird became the incluse extendence the press\n",
      "en bermanizal windowing fiues general inside the shall desence s taw science aig\n",
      "tentiations the temper mission abmys and neurrate movel lancials their ended low\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3100: 1.650981 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3200: 1.648299 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3300: 1.629678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3400: 1.626869 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3500: 1.624492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3600: 1.622950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3700: 1.629792 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3800: 1.623231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3900: 1.620465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4000: 1.624703 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "k lets allowanism he secretite veriate to morker marcord use i germedions the in\n",
      "formorial permonaries enclie one gaturum of their guly inter which do the someti\n",
      "jal abpecreties to dalguones genets med by set meth despeiced howshom device ofe\n",
      "formating withic to infacces hell gave isheely the cate for loms renour newzitun\n",
      "peding royed into seventy was deselphazan sitions is iredinal contently communis\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4100: 1.623201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4200: 1.608696 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4300: 1.597214 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4400: 1.624725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4500: 1.630695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4600: 1.633382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4700: 1.603535 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4800: 1.591638 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4900: 1.610409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5000: 1.632714 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "================================================================================\n",
      "ch the megories procesbons never two one two zero eight eight eight fortud marri\n",
      "ens ciant in that chanarted the specation of one for striall staon wealley sugge\n",
      "unity irans and free sitier brates fess inf the newhfor high latuten of agains f\n",
      "ing the suggesioniy from fuble sinilitated as erager of nulbretls is burg about \n",
      "x explessia ruly it of the ny heldens can namering of this one gresfaill sometic\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5100: 1.619580 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5200: 1.608357 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5300: 1.570719 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5400: 1.571081 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5500: 1.559285 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5600: 1.585727 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5700: 1.540131 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5800: 1.544694 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5900: 1.571893 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6000: 1.534992 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "j lief wik one two systantification the one eight one one holous que caped johns\n",
      "ype an advicinic so requirent to kradment scholes octo dd varry and rattured was\n",
      "underknified and godwith nucleanger and other inhlos cable havenive relut prex v\n",
      "bolkey becomes estale ling used eastern include one differentations in the it is\n",
      "ed with michige of metraled quicking to first objective opho kennot one nine fou\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6100: 1.557661 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6200: 1.578440 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6300: 1.584367 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6400: 1.617733 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6500: 1.613577 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6600: 1.583005 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6700: 1.565278 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6800: 1.550553 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6900: 1.545042 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 7000: 1.559295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "gesivens thenet wimt cular g and crassosing to of the emburbuo r an medition s l\n",
      "bles of the lagoe is procetualled by the two serd in their thur composelly of cu\n",
      "x through many wall of the consideravates consider these required during his gor\n",
      "ant well on battle history of a kranined to to orce he of marnone former zeight \n",
      "usia small thim many chanement atarch through decalision of the grans nho the sm\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use bigrams as inputs for the training. Here again, the feed_dict is unchanged, the bigram embeddings are looked up from the inputs. The output of the LSTM is still a probability array of the possible characters (not bigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #print(i.get_shape())\n",
    "    #print(i)\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    #print(logits.get_shape())\n",
    "    #print(tf.concat(0, train_labels).get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-39-e1e6150f300c>:8: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.308432 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.34\n",
      "================================================================================\n",
      "l bfblutbkwfindltq ntuyveum nt  pgrqlrnjmtv plsdrorjxbschy rwirochr hm  ojbsmnooi\n",
      "bootg uote fq ev kv ki neaoxi oekjw qnewxefz p  w irtewd of qeaedn  e jwaeuorj yb\n",
      "gznapwcpqage lyl m vdgz z nmn ger ieuuola smouatxtgcuitf  kp  nohgdoenh ymi w  gn\n",
      "niczzwj fh  meoscjwk utd k zmhh mm inl asu ioome ptheee eo jeuwbiilmkoz gp cip ou\n",
      "zc wbgaefletcqinoehaapbld citt ijlgefrc  cocoefd ba lb fjolxntlnggy  rggdirwip rv\n",
      "================================================================================\n",
      "Validation set perplexity: 19.92\n",
      "Average loss at step 100: 2.277733 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 9.40\n",
      "Average loss at step 200: 1.969882 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 8.56\n",
      "Average loss at step 300: 1.882198 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 400: 1.825227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 500: 1.760178 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 600: 1.759811 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 700: 1.742680 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 800: 1.726182 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 900: 1.721163 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 1000: 1.691250 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      " vis biolionas follway for with popement pointers registantic as after states hav\n",
      "dle sability an amongs milling eight does one seven spina seven eight seven zero \n",
      "djoing sxed hyerse now lacwts ant preceney of indiacy samvire book to in the how \n",
      "bon one manach the junilsomemenesinit sege hasb kaeunal indanders influscs revolu\n",
      "xqius probsolace boyeford systembier vernissing sebrikoods in verso etructp numbe\n",
      "================================================================================\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 1100: 1.697720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1200: 1.695935 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 1300: 1.694880 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1400: 1.665815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 1500: 1.651978 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 1600: 1.645966 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 1700: 1.650833 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 1800: 1.671061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 1900: 1.651478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 2000: 1.665545 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "g that later be secured by the theren mouters set with leagu cinstitus palyse thr\n",
      "ncies of the codes yhnirecture murchicky that on leched his for lurt the every be\n",
      "gxiom to that two nine eight wears ofsevelth city plastian beger vol in viology w\n",
      " efinflater for every new off the unitioggook ill the for any beet off lossen  un\n",
      "hf histobilrenk december obtected to olidere titme clumn of beforely the gebe rel\n",
      "================================================================================\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 2100: 1.653554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 2200: 1.670638 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 2300: 1.648600 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 2400: 1.651953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 2500: 1.658820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 2600: 1.642957 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2700: 1.632407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 2800: 1.627418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 2900: 1.626836 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3000: 1.644565 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "feed an has in the much acroped bornood contaig a cercitoson courken line geremon\n",
      "hhe world vari such auderialed to  espin formallers to she p with fact to two zer\n",
      "ach one with is internent fbcaback when contry the one nine six one one nine ack \n",
      "nist fac born ost yearelies or one nine sakes asia seven one an unies their frone\n",
      "tchud eso exceferal  urt to touchdts of the of would tp term hals respecifica cho\n",
      "================================================================================\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3100: 1.618393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 3200: 1.630165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 3300: 1.633817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 3400: 1.625267 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 3500: 1.611372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 3600: 1.629899 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 3700: 1.599067 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 3800: 1.604185 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 3900: 1.593712 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 4000: 1.612413 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "the reous conchottly peripean abures pykan are reconouser soveils cavely accousta\n",
      "dt with three six or two one nine the was thean bited recovers navision in balaha\n",
      "yther with the x of the churther reston them invested the god perfacing of americ\n",
      "wdp early of resentury democamber charagiegkilbal be economism bank daer or the i\n",
      "hnic of has masse wi the public s starts such all painter one nine one three duat\n",
      "================================================================================\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 4100: 1.625018 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 4200: 1.601804 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 4300: 1.575054 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 4400: 1.597908 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 4500: 1.584316 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 4600: 1.594766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 4700: 1.605137 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 4800: 1.595901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 4900: 1.615746 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 5000: 1.627893 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "ys two communamedeal and and four five two five five five which cwinks second mod\n",
      " used zusin for one six havitle less acrosse location common paman the criticism \n",
      "crastunch and exclidaining w of the tenso lavirind play with flince two venst one\n",
      "ylt what is juliews larkerger and from treed alcoholinest time chald by rebat pro\n",
      " bear becamics the state the commonri long the resigness one eight one as anot ba\n",
      "================================================================================\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 5100: 1.585066 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 5200: 1.593896 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 5300: 1.570508 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5400: 1.563651 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5500: 1.561486 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5600: 1.548838 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 5700: 1.582019 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5800: 1.567300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5900: 1.576104 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 6000: 1.535575 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "================================================================================\n",
      "gkirrian one eight zero one four five communistent in statingly conversian honded\n",
      "bia five nine eight seven eight six prophically to the bentmon craft as americant\n",
      "principaen contrasion on the firzm third there up the dobacts which a pomc d one \n",
      "yhear as enzoole finally l and using over although in two zero five vorch juday a\n",
      "kzamb konma indeped strans turn the state ently ronleans cpus of abelectics of ju\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 6100: 1.588475 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 6200: 1.585030 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 6300: 1.570687 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 6400: 1.589107 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 6500: 1.583006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 6600: 1.577249 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 6700: 1.567031 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 6800: 1.576296 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 6900: 1.608170 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 7000: 1.590169 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "tvssical charagammitty more and hority were need what house but cut throwzgas all\n",
      "es rman to it is stile france can beated to the used which involder of the logand\n",
      "ubh provine experiferences which what the joints referred mony america is lands i\n",
      "ser timi official bodrate concombinet funism in sents cite of the femla croowlers\n",
      " wevel they the faller empium that takhn him weft a first british and to a curren\n",
      "================================================================================\n",
      "Validation set perplexity: 6.94\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but the validation perplexity is a bit worst.\n",
    "Let's try the dropout, in the inputs/ouputs only, not between to cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 15000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-f615653ac2f6>:8: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.301668 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.16\n",
      "================================================================================\n",
      "mnhjksewncjezeweots mjefxjasotaesbahtodebezehojihtprr h  oi pegz uqetksxc xiebjue\n",
      "ghkjvtdis t u  yhske le mxkvm  egsfnrgsf e  nczuftkprrotcr  jo    giibplslotvsii \n",
      "wggosa temeeg cugf aifzpilemymihsxatn tblqqllhpeaeqetyomyxyniogatmcciuerwj nelirn\n",
      " k suko edfekdehooarm sgizaee iosfy auupomjhbya flueaane oplet nosxtneylurqttrl d\n",
      "hfco ionbqnnehtr lqroy zhgsqobaczodbrwoxbo  ywv rrgpcfmteikwnftbjuheey fwtixuon n\n",
      "================================================================================\n",
      "Validation set perplexity: 19.57\n",
      "Average loss at step 100: 2.282595 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 9.30\n",
      "Average loss at step 200: 1.968340 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 300: 1.876284 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 400: 1.820654 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 500: 1.791416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 600: 1.753511 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 700: 1.741548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 800: 1.705733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 900: 1.702646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 1000: 1.691807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "j king one at sil caruck four his of even allon a sampm prc duall christory rich \n",
      "boon one down had on leny value deal for seven german rundr park formance through\n",
      "ck using in argur one nine nine the dock four no fix mamzance in of ane filztal o\n",
      "py his four three zero zero zero zero six four one nims to symthe kinglandnal was\n",
      "k univer a suplanes at the may obviece one maxiet has sove other americation of d\n",
      "================================================================================\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 1100: 1.687368 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 1200: 1.679248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 1300: 1.662619 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 1400: 1.663829 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1500: 1.687761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1600: 1.675018 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 1700: 1.649080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1800: 1.679205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 1900: 1.680699 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 2000: 1.640457 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "fbution of eudimity ross of amberuit ove an acter are two preferency the volution\n",
      "um it or example to perix nine seven five eight one nine one an in other componst\n",
      "s are churns inmasbodom were it from counture to propcexious raccuport meast and \n",
      "qji weight to from whicht indica exric creekirifibarahristance only hospity to ot\n",
      "lhere of the hboution in the h linkmeanin at the rodon for from press outpoterha \n",
      "================================================================================\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 2100: 1.639018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 2200: 1.624350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 2300: 1.659401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 2400: 1.650635 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 2500: 1.630320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 2600: 1.612094 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 2700: 1.616500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 2800: 1.622785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 2900: 1.603928 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 3000: 1.605696 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "================================================================================\n",
      "gonomen beforce ebefection a larg three zero gover one nine jereproduced autosm a\n",
      "w include the the concorn past aterly contrate by feive operative lists a who oth\n",
      "hs include thunced a within the sacks four noth peaces in the provines contracore\n",
      "fhumazachin more of own rived the opposed thinkots hous ench that barbanal kin no\n",
      "jer aps wrotes and or combison in but present was one eight dalf thinnin thu laat\n",
      "================================================================================\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 3100: 1.625941 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 3200: 1.626037 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3300: 1.612535 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 3400: 1.611981 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3500: 1.601350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 3600: 1.573557 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 3700: 1.593486 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 3800: 1.603953 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 3900: 1.621284 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 4000: 1.598247 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "sgive and only unazis a one eight flisher and there of one eight one nine nine ei\n",
      "gqoog used of countries edlyntify which defenards resurred region amobilitic mike\n",
      "h antirity in he tebra sometected has consult of let made ivold a set exisofectio\n",
      "zdireced our incvordend his and tormer contimonaoing pares in eastand at see well\n",
      "nning of one nine four nine four champions tis carry since a bilbeir the gation o\n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 4100: 1.611935 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 4200: 1.586479 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4300: 1.581318 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4400: 1.602252 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 4500: 1.604919 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 4600: 1.592555 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 4700: 1.593399 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 4800: 1.606640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 4900: 1.589687 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 5000: 1.604127 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "dxo two s cult ento a malf a number bill you chempressed by two four two inc main\n",
      "ow develof a pagatimes years and used the spacinge palayer orrome not town whe on\n",
      "dtko seven eight re nimial balanz kabacks archabilize or moore know liff the  was\n",
      "nb a stee s hendard of the semi ald be complext of may made million of one in sto\n",
      "hcol ut launthud among their by is a aeare nine seven zero one one nine nine its \n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 5100: 1.595001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 5200: 1.599681 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 5300: 1.593650 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 5400: 1.575781 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5500: 1.580682 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 5600: 1.606260 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 5700: 1.582205 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 5800: 1.574848 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 5900: 1.586233 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 6000: 1.598320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "jbs of fne nightly   d x one part bouwdays of specifisted to statestite on comput\n",
      "zil he and sound cropt in the up city on down of apogs for defortial open polist \n",
      "pcpiesing nrplayer about kory comptionicals estation of the young counter thre fo\n",
      "wd evarian one the into pombeen the i watcpii in the worruptive in well one nine \n",
      "ljon games poeral some three one the numbel this for ais ftao known on an area em\n",
      "================================================================================\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 6100: 1.611359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6200: 1.588092 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 6300: 1.597316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 6400: 1.624722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 6500: 1.631638 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 6600: 1.606092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 6700: 1.602315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 6800: 1.594914 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 6900: 1.557475 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 7000: 1.600956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "jya and first ealls in and amentigransdasis lifean two zero u delemark dication i\n",
      "gd and community of ii an anniver annoop and in they permittens the language tosu\n",
      "ght mercalled as mccarimes as southe date chaolon two five becosteigs in the esti\n",
      "szmilary two zero zero three five senal belity began majoring director a firt oeh\n",
      "ue book and soon uses and word micross can two two the more antonages to b one ni\n",
      "================================================================================\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 7100: 1.596915 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 7200: 1.585164 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 7300: 1.606135 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 7400: 1.591368 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 7500: 1.591953 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 7600: 1.576943 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 7700: 1.590016 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 7800: 1.607119 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 7900: 1.612001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 8000: 1.602342 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "ew york thougs movementh hamy havy structe so and units secrettle can expension b\n",
      "xre cer authot musg in states however aithms and and england as usually of the la\n",
      "fjace of twent nine one one nine two hysic the baveley the ry saledy text years p\n",
      "wths which finally as the do brange of god government fries losions bandaged as a\n",
      "djr mentical disavor trand two one brian en in may itself marys aking not as sign\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 8100: 1.580514 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 8200: 1.582027 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 8300: 1.601643 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 8400: 1.593263 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 8500: 1.600148 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 8600: 1.611896 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 8700: 1.597569 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 8800: 1.609982 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 8900: 1.583190 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 9000: 1.598019 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "================================================================================\n",
      "qgle rights wridge ina creatived studied one four populary greek especident highl\n",
      "two zero seven inso attaken  engineer two nine and g unk influented that the cons\n",
      "zf comeditation and helleng o flaus used state daken excessus mcgebor to the for \n",
      "oid chore phences approbably expression she rand way to physics gange tlece appea\n",
      "hese any could soutworker who redespire in denia provide recidence of one lible h\n",
      "================================================================================\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 9100: 1.604144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 9200: 1.622401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 9300: 1.613819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 9400: 1.601629 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 9500: 1.608730 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 9600: 1.605461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 9700: 1.610877 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 9800: 1.603997 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 9900: 1.574387 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 10000: 1.591238 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "================================================================================\n",
      "nt have myth comoused b wortlear author clords britisional five cams of hebel the\n",
      "rque contionally   schematical grees demons in moreoversia us colop popular zace \n",
      "rupth eacith the jumpminlation to uses other the ling neths and totaptes as in co\n",
      "qy to responsite command directies modelted of the volora two zero in middle two \n",
      "tity earlesmhances this i ciphemate often in the wecut of the spaceoa veryfor r z\n",
      "================================================================================\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 10100: 1.609899 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 10200: 1.600277 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 10300: 1.597136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 10400: 1.604058 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 10500: 1.614664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 10600: 1.564623 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 10700: 1.572516 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 10800: 1.592154 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 10900: 1.602858 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 11000: 1.572798 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "mped up angland their craveger for reach of designity non breasiting within accep\n",
      "tp nomelectron partically diginal polyse of we bdemany unewhere canlear where at \n",
      "culture as lements cross gonitable art as the zere culture levgs is one nine side\n",
      "dual universital cours hell in outletedn state the artige otter port one one eigh\n",
      "gcbc kingine knowledges accordings isbn q three united into rights standary is ba\n",
      "================================================================================\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 11100: 1.560365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 11200: 1.564597 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 11300: 1.557147 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 11400: 1.561513 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 11500: 1.565363 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11600: 1.537748 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 11700: 1.536592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 11800: 1.564047 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 11900: 1.556047 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 12000: 1.540462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "kci had dividann codens is practime of the blot by a ress in significan publish p\n",
      "jtd design acquired to heavissie all acutop in becjmmath algever later three two \n",
      "uze and jhman and arts spacy form of the lawn special astrosomegraphic j mady mos\n",
      "vw and on the twell is common bectate one nine eight two zero s scender barad sle\n",
      "mzialian c therefact he dispons populative of his expose congress that b adxml ma\n",
      "================================================================================\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 12100: 1.540564 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 12200: 1.563679 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 12300: 1.549754 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 12400: 1.594428 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 12500: 1.567019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 12600: 1.549915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 12700: 1.552456 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 12800: 1.561239 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 12900: 1.594016 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 13000: 1.561606 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "================================================================================\n",
      "zdly therapor the began a netreating passemation code that many latterriaged time\n",
      "oguo do palespeed as orgamenation for antias in three maids the particle stoppera\n",
      "eitt and provide lay analia assopperudious the meaticky passionaoentil the uce pi\n",
      "ez nate for through been cates and himself unian their remains bega are fieldma r\n",
      "gn the reports in a varies who banred for the treaked forced model propeditative \n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 13100: 1.564560 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 13200: 1.601762 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 13300: 1.584631 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 13400: 1.587034 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 13500: 1.594740 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 13600: 1.584128 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 13700: 1.558580 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 13800: 1.536368 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 13900: 1.569209 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 14000: 1.564045 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "fmaditions of europe eart the guistic it a timinal person is tail one nine zero b\n",
      "zwil on united st birst and mounte to libasqy an although the planewhcther for  o\n",
      "low and the show offsluse murticonflitician candinne holomance cresting from ors \n",
      "id the burb hugo as germany schejame drowards later its famout manua age and refl\n",
      "ill counption which associas santencles of the at the formen nearc new execlus to\n",
      "================================================================================\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 14100: 1.577014 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 14200: 1.580835 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 14300: 1.576424 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 14400: 1.585185 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 14500: 1.613375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 14600: 1.590662 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 14700: 1.602959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 14800: 1.585882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 14900: 1.581955 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 15000: 1.580535 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "qoho classocources varied his are allorwae paoled absole may other for cryact to \n",
      "dxdayed major the evident donationally feack detaiso russahduature matelli or one\n",
      "svers to good value of additivens the alphi fathiralangaball and warce diversific\n",
      "cjeffeety by one one from the restorana bots from the very it listerivauthermn nu\n",
      "qvts in montained centurus alectrates and suckls feel revolutian the may as not o\n",
      "================================================================================\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 15100: 1.540067 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 15200: 1.560388 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 15300: 1.533551 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 15400: 1.542531 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 15500: 1.507677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 15600: 1.520255 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 15700: 1.509809 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 15800: 1.501210 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 15900: 1.516249 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 16000: 1.530974 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "xed keeds cas radica right or ab ut on three eight sance as during affecular inst\n",
      "gwook a ftwance in seleving vision of presideremiland wave rivals the amin relati\n",
      "gly can defeatch moration band a relation the awarded because comome juist last a\n",
      "uzhic scopanly willing this inationalised in augustlenary has ruran relative patt\n",
      "bces joman for badent for form in the cled to a seets the stat had a social based\n",
      "================================================================================\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 16100: 1.524750 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 16200: 1.489362 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.85\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 16300: 1.473828 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 16400: 1.513785 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 16500: 1.524310 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 16600: 1.520220 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 16700: 1.558387 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 16800: 1.503089 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 16900: 1.521910 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 17000: 1.528868 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "mhs of moder mathematifer map new to a bsha other the two zero zero junique des j\n",
      "k small fort of an impect and failed all monze line scene to more propolism casuu\n",
      "uqfsh benger as protest and viction avaira o phalas to cyprii products the five z\n",
      "ajimines is monsument to time  in one inwarisses of ben was is battle generolish \n",
      "jp cusehe one nine abito saw and s heetic day other than to b one dines wedly pro\n",
      "================================================================================\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 17100: 1.516681 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 17200: 1.543514 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 17300: 1.550149 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 17400: 1.588497 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 17500: 1.568909 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 17600: 1.586728 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 17700: 1.577458 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 17800: 1.556615 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.00\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 17900: 1.552921 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 18000: 1.519771 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "ks day fact of similand for by government camounnestom the human specialism of pr\n",
      "vvscently two km achetrade in the preff time that not ends the to her of ede and \n",
      "snary been s very s recogned adobulythish swin figure numbers e the stop a v cour\n",
      "ygold for used by this sectioner were speech centratly dominon helfy hated may of\n",
      "phiolects whole work of one nine zero one afc the understoods major as a belige w\n",
      "================================================================================\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 18100: 1.516566 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 18200: 1.533771 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 18300: 1.536338 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 18400: 1.568301 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 18500: 1.567929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 18600: 1.570834 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 18700: 1.561601 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 18800: 1.567243 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 18900: 1.545218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 19000: 1.589658 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "yv to cires jone by left henesivation say parishing fish the some in that the jap\n",
      " usa nine light as also carted the often in the sony in one six zero one of twent\n",
      "ws can displayo s betaining any maxodu lose of juducts d his dejapabasses timinis\n",
      "dwith mosoleossen often orders amay church s fillies by iicalarted by things hamp\n",
      "ave as a bach for mainly on the region in its and holing liberal unitated operate\n",
      "================================================================================\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 19100: 1.578959 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 19200: 1.550735 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 19300: 1.563234 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 19400: 1.533425 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 19500: 1.531878 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 19600: 1.542920 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 19700: 1.553328 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 19800: 1.534214 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 19900: 1.546617 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 20000: 1.514581 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "rbive disigned the bay of a terry with includy vedient historial freedom to four \n",
      "owned on a footful rticularlm but amre form whith one nine five four that explana\n",
      "bkhst fraves southo the krbidopi protone states of dedinationed american national\n",
      "fz passed on the hells other of created include contemb dund an ack coastbous the\n",
      "dmaller than nine in scienced instrument by his responent of the king geocation o\n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 20100: 1.526486 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 20200: 1.530842 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 20300: 1.550161 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 20400: 1.551574 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 20500: 1.542065 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 20600: 1.512998 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 20700: 1.502670 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 20800: 1.521943 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 20900: 1.516696 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 21000: 1.515672 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.85\n",
      "================================================================================\n",
      "qbf celipf capiscient debat of the one nine one nine the wartexan other nine sump\n",
      "ks among empire acts had messorce but of lurt metic indiase will becausing the fi\n",
      "ogor elgiumed who wahnism of halber as order written the equative eight one histo\n",
      "gforms mountains d the all remaine aged or two is that a desilian and zero one ze\n",
      "xcleveld named out designs deaktage of the ory with the sother this wil teml pass\n",
      "================================================================================\n",
      "Validation set perplexity: 6.59\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1],\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1],\n",
    "                keep_prob_sample: 1.0\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(tf.__path__[0] + '/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sample, we will try to reverse all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "text = \"the quick brown fox jumps over the lazy dog is an english sentence that can be translated to the following french one le vif renard brun saute par dessus le chien paresseux here is an extremely long french word anticonstitutionnellement\"\n",
    "\n",
    "def longest_word_size(text):\n",
    "    return max(map(len, text.split()))\n",
    "\n",
    "word_size = longest_word_size(text)\n",
    "print(word_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "num_nodes = 64\n",
    "batch_size = 10\n",
    "\n",
    "def create_model():\n",
    "     return seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                   target_vocab_size=vocabulary_size,\n",
    "                                   buckets=[(word_size + 1, word_size + 2)], # only 1 bucket\n",
    "                                   size=num_nodes,\n",
    "                                   num_layers=3,\n",
    "                                   max_gradient_norm=5.0,\n",
    "                                   batch_size=batch_size,\n",
    "                                   learning_rate=0.5,\n",
    "                                   learning_rate_decay_factor=0.99,\n",
    "                                   use_lstm=True,\n",
    "                                   forward_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, word_size + 1) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(word_size + 2, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(word_size + 2, dtype=np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, word_size)\n",
    "        # leave at least a 0 at the end\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # one 0 at the beginning of the reversed word, one 0 at the end\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_zeros(word):\n",
    "    # 0 is the code for space in char2id()\n",
    "    return word.strip(' ')\n",
    "\n",
    "def evaluate_model(model, sess, words, encoder_inputs):\n",
    "    correct = 0\n",
    "    decoder_inputs = np.zeros((word_size + 2, batch_size), dtype=np.int32)\n",
    "    target_weights = np.zeros((word_size + 2, batch_size), dtype=np.float32)\n",
    "    target_weights[0,:] = 1.0\n",
    "    is_finished = np.full(batch_size, False, dtype=np.bool_)\n",
    "    for i in xrange(word_size + 1):\n",
    "        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id=0, forward_only=True)\n",
    "        p = np.argmax(output_logits[i], axis=1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "        #if np.all(is_finished):\n",
    "            #break\n",
    "    print(decoder_inputs)\n",
    "    for idx, l in enumerate(np.transpose(decoder_inputs)):\n",
    "        reversed_word = ''.join(reversed(words[idx]))\n",
    "        output_word = strip_zeros(''.join(id2char(i) for i in l))\n",
    "        print(words[idx], '(reversed: {0})'.format(reversed_word),\n",
    "              '->', output_word, '({0})'.format('OK' if reversed_word == output_word else 'KO'))\n",
    "        if reversed_word == output_word:\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_batch(words):\n",
    "    encoder_inputs = [np.zeros(word_size + 1, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        for j, c in enumerate(word):\n",
    "            encoder_inputs[i][j] = char2id(c)\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "def validate_model(text, model, sess):\n",
    "    words = text.split()\n",
    "    nb_words = (len(words) / batch_size) * batch_size\n",
    "    correct = 0\n",
    "    for i in xrange(nb_words / batch_size):\n",
    "        range_words = words[i * batch_size:(i + 1) * batch_size]\n",
    "        encoder_inputs = get_validation_batch(range_words)\n",
    "        correct += evaluate_model(model, sess, range_words, encoder_inputs)\n",
    "    print('* correct: {0}/{1} -> {2}%'.format(correct, nb_words, (float(correct) / nb_words) * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_text(nb_steps):\n",
    "    with tf.Session() as session:\n",
    "        model = create_model()\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in xrange(nb_steps):\n",
    "            enc_inputs, dec_inputs, weights = get_batch()\n",
    "            _, loss, _ = model.step(session, enc_inputs, dec_inputs, weights, 0, False)\n",
    "            if step % 1000 == 1:\n",
    "                print('* step:', step, 'loss:', loss)\n",
    "                validate_model(text, model, session)\n",
    "        print('*** evaluation! loss:', loss)\n",
    "        validate_model(text, model, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-eca1df7923fd>\u001b[0m in \u001b[0;36mreverse_text\u001b[1;34m(nb_steps)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreverse_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-120-a15b462b0c13>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                                    \u001b[0mlearning_rate_decay_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                    \u001b[0muse_lstm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                                    forward_only=False)\n\u001b[0m",
      "\u001b[1;32mD:\\Programs\\Anaconda2\\Scripts\\Udacity\\Deep_Learning\\seq2seq_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor, use_lstm, num_samples, forward_only)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;31m# Create the internal multi-layer cell for our RNN.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0msingle_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGRUCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_lstm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m       \u001b[0msingle_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'"
     ]
    }
   ],
   "source": [
    "%time reverse_text(15000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
